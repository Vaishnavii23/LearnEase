{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dfeff04",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "639b771d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slide 'Conclusion' has importance rank: 1\n",
      "Slide 'Supervised Learning' has importance rank: 2\n",
      "Slide 'Unsupervised Learning' has importance rank: 3\n",
      "Slide 'Introduction to Machine Learning' has importance rank: 4\n",
      "Slide 'Applications of Machine Learning' has importance rank: 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/vaishnavikamisetti/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vaishnavikamisetti/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample slide content\n",
    "slides = [\n",
    "    \"Introduction to Machine Learning\",\n",
    "    \"Supervised Learning\",\n",
    "    \"Unsupervised Learning\",\n",
    "    \"Applications of Machine Learning\",\n",
    "    \"Conclusion\"\n",
    "]\n",
    "\n",
    "# Preprocessing\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "processed_slides = []\n",
    "for slide in slides:\n",
    "    words = word_tokenize(slide.lower())\n",
    "    filtered_words = [stemmer.stem(word) for word in words if word not in stop_words and word.isalnum()]\n",
    "    processed_slides.append(' '.join(filtered_words))\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_slides)\n",
    "\n",
    "# Calculate importance scores for each slide\n",
    "importance_scores = tfidf_matrix.sum(axis=1).flatten()\n",
    "\n",
    "# Convert importance_scores to NumPy array\n",
    "importance_scores = np.array(importance_scores)\n",
    "\n",
    "# Assign slide numbers based on importance\n",
    "sorted_indices = importance_scores.argsort()[::-1].tolist()[0]  # Extract inner list\n",
    "slide_numbers = {slide: i+1 for i, slide in enumerate(sorted_indices)}\n",
    "\n",
    "# Print slide numbers\n",
    "for slide, number in slide_numbers.items():\n",
    "    print(f\"Slide '{slides[slide]}' has importance rank: {number}\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3b7faf",
   "metadata": {},
   "source": [
    "# NER\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d24c0749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slide: Introduction to Machine Learning\n",
      "Entities: []\n",
      "\n",
      "Slide: Supervised Learning\n",
      "Entities: []\n",
      "\n",
      "Slide: Unsupervised Learning\n",
      "Entities: ['Unsupervised Learning']\n",
      "\n",
      "Slide: Applications of Machine Learning\n",
      "Entities: ['Applications of Machine Learning']\n",
      "\n",
      "Slide: Conclusion\n",
      "Entities: []\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the smaller English NER model\n",
    "nlp_sm = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Load the larger English NER model\n",
    "nlp_lg = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Sample slides\n",
    "slides = [\n",
    "    \"Introduction to Machine Learning\",\n",
    "    \"Supervised Learning\",\n",
    "    \"Unsupervised Learning\",\n",
    "    \"Applications of Machine Learning\",\n",
    "    \"Conclusion\"\n",
    "]\n",
    "\n",
    "# Function to extract entities using NER model\n",
    "def extract_entities(nlp_model, text):\n",
    "    doc = nlp_model(text)\n",
    "    entities = [ent.text for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "# Extract entities using both NER models\n",
    "all_entities = []\n",
    "for slide in slides:\n",
    "    entities_sm = extract_entities(nlp_sm, slide)\n",
    "    entities_lg = extract_entities(nlp_lg, slide)\n",
    "    all_entities.extend(entities_sm)\n",
    "    all_entities.extend(entities_lg)\n",
    "\n",
    "# Remove duplicates\n",
    "unique_entities = list(set(all_entities))\n",
    "\n",
    "# Print entities for each slide\n",
    "for slide in slides:\n",
    "    entities_sm = extract_entities(nlp_sm, slide)\n",
    "    entities_lg = extract_entities(nlp_lg, slide)\n",
    "    combined_entities = list(set(entities_sm + entities_lg))\n",
    "    print(f\"Slide: {slide}\")\n",
    "    print(f\"Entities: {combined_entities}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0027e85",
   "metadata": {},
   "source": [
    "# TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03e7f36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords: ['learning', 'machine', 'introduction', 'supervised', 'unsupervised']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "\n",
    "# Sample slide content\n",
    "slides = [\n",
    "    \"Introduction to Machine Learning\",\n",
    "    \"Supervised Learning\",\n",
    "    \"Unsupervised Learning\",\n",
    "    \"Applications of Machine Learning\",\n",
    "    \"Conclusion\"\n",
    "]\n",
    "\n",
    "# Tokenize and preprocess slide content\n",
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = [word.lower() for slide in slides for word in word_tokenize(slide) if word.isalnum()]\n",
    "filtered_words = [word for word in word_tokens if word not in stop_words]\n",
    "\n",
    "# Calculate word frequency\n",
    "word_freq = Counter(filtered_words)\n",
    "\n",
    "# Calculate TextRank scores for each word\n",
    "num_words = len(filtered_words)\n",
    "word_scores = {word: word_freq[word] / num_words for word in word_freq}\n",
    "\n",
    "# Extract keywords based on TextRank scores\n",
    "num_keywords = 5  # Number of keywords to extract\n",
    "keywords = [word for word, score in sorted(word_scores.items(), key=lambda x: x[1], reverse=True)[:num_keywords]]\n",
    "\n",
    "print(\"Keywords:\", keywords)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc76526",
   "metadata": {},
   "source": [
    "# Rake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d40e363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rake-nltk\n",
      "  Downloading rake_nltk-1.0.6-py3-none-any.whl (9.1 kB)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in /Users/vaishnavikamisetti/opt/anaconda3/lib/python3.9/site-packages (from rake-nltk) (3.7)\n",
      "Requirement already satisfied: joblib in /Users/vaishnavikamisetti/opt/anaconda3/lib/python3.9/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (1.2.0)\n",
      "Requirement already satisfied: click in /Users/vaishnavikamisetti/opt/anaconda3/lib/python3.9/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/vaishnavikamisetti/opt/anaconda3/lib/python3.9/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/vaishnavikamisetti/opt/anaconda3/lib/python3.9/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (4.64.1)\n",
      "Installing collected packages: rake-nltk\n",
      "Successfully installed rake-nltk-1.0.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rake-nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a9cd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords for slide 'Introduction to Machine Learning': [(4.0, 'machine learning'), (1.0, 'introduction')]\n",
      "Keywords for slide 'Supervised Learning': [(4.0, 'supervised learning')]\n",
      "Keywords for slide 'Unsupervised Learning': [(4.0, 'unsupervised learning')]\n",
      "Keywords for slide 'Applications of Machine Learning': [(4.0, 'machine learning'), (1.0, 'applications')]\n",
      "Keywords for slide 'Conclusion': [(1.0, 'conclusion')]\n"
     ]
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "\n",
    "# Sample slide content\n",
    "slides = [\n",
    "    \"Introduction to Machine Learning\",\n",
    "    \"Supervised Learning\",\n",
    "    \"Unsupervised Learning\",\n",
    "    \"Applications of Machine Learning\",\n",
    "    \"Conclusion\"\n",
    "]\n",
    "\n",
    "# Initialize RAKE\n",
    "r = Rake()\n",
    "\n",
    "# Extract keywords from each slide\n",
    "for slide in slides:\n",
    "    r.extract_keywords_from_text(slide)\n",
    "    keywords_with_scores = r.get_ranked_phrases_with_scores()\n",
    "    print(f\"Keywords for slide '{slide}': {keywords_with_scores}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc4a7ce",
   "metadata": {},
   "source": [
    "# Topic Modelling - LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9dcd9abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slide 'Introduction to Machine Learning' has importance rank: 1\n",
      "Slide 'Supervised Learning' has importance rank: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "\n",
    "# Sample slide content\n",
    "slides = [\n",
    "    \"Introduction to Machine Learning\",\n",
    "    \"Supervised Learning\",\n",
    "    \"Unsupervised Learning\",\n",
    "    \"Applications of Machine Learning\",\n",
    "    \"Conclusion\"\n",
    "]\n",
    "\n",
    "# Convert slides to a document-term matrix\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "doc_term_matrix = vectorizer.fit_transform(slides)\n",
    "\n",
    "# Define the number of topics\n",
    "num_topics = 2\n",
    "\n",
    "# Apply LDA\n",
    "lda = LatentDirichletAllocation(n_components=num_topics, random_state=42)\n",
    "lda.fit(doc_term_matrix)\n",
    "\n",
    "# Get topic-term matrix\n",
    "topic_term_matrix = lda.components_\n",
    "\n",
    "# Assign importance scores based on topics\n",
    "topic_importance = np.sum(topic_term_matrix, axis=1)\n",
    "\n",
    "# Assign slide numbers based on importance\n",
    "sorted_indices = topic_importance.argsort()[::-1]\n",
    "slide_numbers = {slide: i+1 for i, slide in enumerate(sorted_indices)}\n",
    "\n",
    "# Print slide numbers\n",
    "for slide, number in slide_numbers.items():\n",
    "    print(f\"Slide '{slides[slide]}' has importance rank: {number}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40232540",
   "metadata": {},
   "source": [
    "# Topic Modelling - NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "233ea949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slide 'Introduction to Machine Learning' has importance rank: 1\n",
      "Slide 'Supervised Learning' has importance rank: 2\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Sample slide content\n",
    "slides = [\n",
    "    \"Introduction to Machine Learning\",\n",
    "    \"Supervised Learning\",\n",
    "    \"Unsupervised Learning\",\n",
    "    \"Applications of Machine Learning\",\n",
    "    \"Conclusion\"\n",
    "]\n",
    "\n",
    "# Convert slides to TF-IDF matrix\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "tfidf_matrix = vectorizer.fit_transform(slides)\n",
    "\n",
    "# Define the number of topics\n",
    "num_topics = 2\n",
    "\n",
    "# Apply NMF\n",
    "nmf = NMF(n_components=num_topics, random_state=42)\n",
    "nmf.fit(tfidf_matrix)\n",
    "\n",
    "# Get topic-term matrix\n",
    "topic_term_matrix = nmf.components_\n",
    "\n",
    "# Assign importance scores based on topics\n",
    "topic_importance = np.sum(topic_term_matrix, axis=1)\n",
    "\n",
    "# Assign slide numbers based on importance\n",
    "sorted_indices = topic_importance.argsort()[::-1]\n",
    "slide_numbers = {slide: i+1 for i, slide in enumerate(sorted_indices)}\n",
    "\n",
    "# Print slide numbers\n",
    "for slide, number in slide_numbers.items():\n",
    "    print(f\"Slide '{slides[slide]}' has importance rank: {number}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afd56ef",
   "metadata": {},
   "source": [
    "# KeyWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f86aa6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importance of Slide 1: 2\n",
      "Importance of Slide 2: 2\n",
      "Importance of Slide 3: 1\n",
      "Importance of Slide 4: 1\n",
      "Importance of Slide 5: 2\n"
     ]
    }
   ],
   "source": [
    "# Sample slide content\n",
    "slides = [\n",
    "    \"Introduction to Machine Learning: This slide provides an overview of machine learning concepts.\",\n",
    "    \"Supervised Learning: This slide discusses the principles of supervised learning algorithms.\",\n",
    "    \"Algorithm: Here, we delve into the details of the K-nearest neighbors algorithm.\",\n",
    "    \"Applications: Various applications of machine learning in real-world scenarios are covered in this slide.\",\n",
    "    \"Conclusion: A summary of key points and findings is presented in this final slide.\"\n",
    "]\n",
    "\n",
    "# List of keywords\n",
    "keywords = [\n",
    "    \"Equation\", \"Formula\", \"Calculation\", \"Derivation\", \"Theorem\", \"Law\", \"Principle\", \"Axiom\", \"Postulate\",\n",
    "    \"Application\", \"Example\", \"Illustration\", \"Use case\", \"Method\", \"Procedure\", \"Algorithm\", \"Definition\",\n",
    "    \"Explanation\", \"Overview\", \"Introduction\", \"Analysis\", \"Comparison\", \"Limitation\", \"Challenge\",\n",
    "    \"Result\", \"Conclusion\", \"Observation\", \"Summary\"\n",
    "]\n",
    "\n",
    "# Function to calculate importance of each slide\n",
    "def calculate_importance(slide_content):\n",
    "    importance_score = 0\n",
    "    for keyword in keywords:\n",
    "        if keyword.lower() in slide_content.lower():\n",
    "            importance_score += 1\n",
    "    return importance_score\n",
    "\n",
    "# Calculate importance scores for each slide\n",
    "slide_importance = [calculate_importance(slide) for slide in slides]\n",
    "\n",
    "# Print importance scores for each slide\n",
    "for i, importance in enumerate(slide_importance, 1):\n",
    "    print(f\"Importance of Slide {i}: {importance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4bbd4b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-pptx\n",
      "  Downloading python_pptx-0.6.23-py3-none-any.whl (471 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: Pillow>=3.3.2 in /Users/vaishnavikamisetti/opt/anaconda3/lib/python3.9/site-packages (from python-pptx) (9.2.0)\n",
      "Requirement already satisfied: lxml>=3.1.0 in /Users/vaishnavikamisetti/opt/anaconda3/lib/python3.9/site-packages (from python-pptx) (4.9.1)\n",
      "Requirement already satisfied: XlsxWriter>=0.5.7 in /Users/vaishnavikamisetti/opt/anaconda3/lib/python3.9/site-packages (from python-pptx) (3.0.3)\n",
      "Installing collected packages: python-pptx\n",
      "Successfully installed python-pptx-0.6.23\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install python-pptx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db7d777a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Importance of Slide 1: 1.0\n",
      "Hybrid Importance of Slide 2: 2.0\n",
      "Hybrid Importance of Slide 3: 2.0\n",
      "Hybrid Importance of Slide 4: 1.0000000000000002\n",
      "Hybrid Importance of Slide 5: 1.0\n",
      "Hybrid Importance of Slide 6: 1.0\n",
      "Hybrid Importance of Slide 7: 1.0\n",
      "Hybrid Importance of Slide 8: 1.0\n",
      "Hybrid Importance of Slide 9: 1.0\n",
      "Hybrid Importance of Slide 10: 1.0\n",
      "Hybrid Importance of Slide 11: 1.0\n",
      "Hybrid Importance of Slide 12: 1.0\n",
      "Hybrid Importance of Slide 13: 2.0\n",
      "Hybrid Importance of Slide 14: 0.9999999999999999\n",
      "Hybrid Importance of Slide 15: 1.0\n",
      "Hybrid Importance of Slide 16: 1.0\n",
      "Hybrid Importance of Slide 17: 1.0\n",
      "Hybrid Importance of Slide 18: 0.9999999999999999\n",
      "Hybrid Importance of Slide 19: 1.0\n",
      "Hybrid Importance of Slide 20: 1.0\n",
      "Hybrid Importance of Slide 21: 2.0\n",
      "Hybrid Importance of Slide 22: 1.0\n",
      "Hybrid Importance of Slide 23: 1.0\n",
      "Hybrid Importance of Slide 24: 0.9999999999999999\n",
      "Hybrid Importance of Slide 25: 1.0\n",
      "Hybrid Importance of Slide 26: 1.0\n",
      "Hybrid Importance of Slide 27: 1.0\n",
      "Hybrid Importance of Slide 28: 2.0\n",
      "Hybrid Importance of Slide 29: 3.0\n",
      "Hybrid Importance of Slide 30: 2.0\n",
      "Hybrid Importance of Slide 31: 2.0\n",
      "Hybrid Importance of Slide 32: 0.9999999999999999\n",
      "Hybrid Importance of Slide 33: 1.0\n",
      "Hybrid Importance of Slide 34: 1.0\n",
      "Hybrid Importance of Slide 35: 1.0\n",
      "Hybrid Importance of Slide 36: 1.0\n",
      "Hybrid Importance of Slide 37: 1.0\n",
      "Hybrid Importance of Slide 38: 1.0\n",
      "Hybrid Importance of Slide 39: 1.0\n",
      "Hybrid Importance of Slide 40: 2.0\n",
      "Hybrid Importance of Slide 41: 2.0\n",
      "Hybrid Importance of Slide 42: 1.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pptx\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "\n",
    "# Function to extract text from PowerPoint slides\n",
    "def extract_text_from_pptx(pptx_file):\n",
    "    text = []\n",
    "    prs = pptx.Presentation(pptx_file)\n",
    "    for slide in prs.slides:\n",
    "        slide_text = []\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\"):\n",
    "                slide_text.append(shape.text)\n",
    "        text.append(\"\\n\".join(slide_text))\n",
    "    return text\n",
    "\n",
    "# Function to calculate importance of each slide using LDA\n",
    "def calculate_importance_lda(slides):\n",
    "    vectorizer = CountVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "    X = vectorizer.fit_transform(slides)\n",
    "    lda_model = LatentDirichletAllocation(n_components=2, random_state=42)\n",
    "    lda_output = lda_model.fit_transform(X)\n",
    "    importance_scores = np.sum(lda_output, axis=1)\n",
    "    return importance_scores\n",
    "\n",
    "# Function to calculate importance of each slide using manual keywords\n",
    "def calculate_importance_keywords(slides, keywords):\n",
    "    importance_scores = []\n",
    "    for slide in slides:\n",
    "        score = sum(1 for keyword in keywords if keyword.lower() in slide.lower())\n",
    "        importance_scores.append(score)\n",
    "    return importance_scores\n",
    "\n",
    "# Path to the PowerPoint file\n",
    "pptx_path = \"/Users/vaishnavikamisetti/Desktop/NLP.pptx\"\n",
    "\n",
    "# List of keywords\n",
    "keywords = [\n",
    "    \"Equation\", \"Formula\", \"Calculation\", \"Derivation\", \"Theorem\",\n",
    "    \"Law\",\"Principle\", \"Axiom\", \"Postulate\",\n",
    "    \"Application\", \"Example\", \"Illustration\", \"Use case\", \"Method\",\n",
    "    \"Procedure\", \"Algorithm\", \"Definition\",\n",
    "    \"Explanation\", \"Overview\", \"Introduction\", \"Analysis\", \n",
    "    \"Comparison\", \"Limitation\", \"Challenge\",\n",
    "    \"Result\", \"Conclusion\", \"Observation\", \"Summary\"\n",
    "]\n",
    "\n",
    "# Extract text from PowerPoint slides\n",
    "slides_text = extract_text_from_pptx(pptx_path)\n",
    "\n",
    "# Calculate importance scores for each slide using LDA\n",
    "importance_scores_lda = calculate_importance_lda(slides_text)\n",
    "\n",
    "# Calculate importance scores for each slide using manual keywords\n",
    "importance_scores_keywords = calculate_importance_keywords(slides_text, keywords)\n",
    "\n",
    "# Combine importance scores from both approaches\n",
    "hybrid_importance_scores = np.add(importance_scores_lda, importance_scores_keywords)\n",
    "\n",
    "# Print hybrid importance scores for each slide\n",
    "for i, importance in enumerate(hybrid_importance_scores, 1):\n",
    "    print(f\"Hybrid Importance of Slide {i}: {importance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8defdc7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Importance of Slide 1: 1.0\n",
      "Hybrid Importance of Slide 2: 1.0\n",
      "Hybrid Importance of Slide 3: 1.0\n",
      "Hybrid Importance of Slide 4: 0.9999999999999999\n",
      "Hybrid Importance of Slide 5: 2.0\n",
      "Hybrid Importance of Slide 6: 1.0\n",
      "Hybrid Importance of Slide 7: 2.0\n",
      "Hybrid Importance of Slide 8: 1.0\n",
      "Hybrid Importance of Slide 9: 2.0\n",
      "Hybrid Importance of Slide 10: 1.0\n",
      "Hybrid Importance of Slide 11: 1.0\n",
      "Hybrid Importance of Slide 12: 1.0\n",
      "Hybrid Importance of Slide 13: 2.0\n",
      "Hybrid Importance of Slide 14: 3.0\n",
      "Hybrid Importance of Slide 15: 2.0\n",
      "Hybrid Importance of Slide 16: 1.0\n",
      "Hybrid Importance of Slide 17: 1.0\n",
      "Hybrid Importance of Slide 18: 1.0\n",
      "Hybrid Importance of Slide 19: 2.0\n",
      "Hybrid Importance of Slide 20: 3.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pptx\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import numpy as np\n",
    "\n",
    "# Function to extract text from PowerPoint slides\n",
    "def extract_text_from_pptx(pptx_file):\n",
    "    text = []\n",
    "    prs = pptx.Presentation(pptx_file)\n",
    "    for slide in prs.slides:\n",
    "        slide_text = []\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\"):\n",
    "                slide_text.append(shape.text)\n",
    "        text.append(\"\\n\".join(slide_text))\n",
    "    return text\n",
    "\n",
    "# Function to calculate importance of each slide using LDA\n",
    "def calculate_importance_lda(slides):\n",
    "    vectorizer = CountVectorizer(max_df=0.8, min_df=2, stop_words='english')\n",
    "    X = vectorizer.fit_transform(slides)\n",
    "    lda_model = LatentDirichletAllocation(n_components=2, random_state=42)\n",
    "    lda_output = lda_model.fit_transform(X)\n",
    "    importance_scores = np.sum(lda_output, axis=1)\n",
    "    return importance_scores\n",
    "\n",
    "# Function to analyze the structural elements and calculate importance scores\n",
    "def calculate_importance_structure(slides):\n",
    "    importance_scores = []\n",
    "    for slide in slides:\n",
    "        # Analyze structural elements related to the topic\n",
    "        num_headings = slide.count(\"\\n\\n\") if \"topic\" in slide.lower() else 0  # Assuming two consecutive newlines indicate a heading\n",
    "        num_bullet_points = slide.count(\"\\n•\") if \"topic\" in slide.lower() else 0  # Counting bullet points\n",
    "        \n",
    "        # Calculate importance score based on structural elements\n",
    "        score = num_headings + num_bullet_points\n",
    "        importance_scores.append(score)\n",
    "    return importance_scores\n",
    "\n",
    "# Manual defined keywords\n",
    "keywords = [\n",
    "    \"Equation\", \"Formula\", \"Calculation\", \"Derivation\", \"Theorem\", \"Law\", \"Principle\", \"Axiom\", \"Postulate\",\n",
    "    \"Application\", \"Example\", \"Illustration\", \"Use case\", \"Method\", \"Procedure\", \"Algorithm\", \"Definition\",\n",
    "    \"Explanation\", \"Overview\", \"Introduction\", \"Analysis\", \"Comparison\", \"Limitation\", \"Challenge\",\n",
    "    \"Result\", \"Conclusion\", \"Observation\", \"Summary\"\n",
    "]\n",
    "\n",
    "# Path to the PowerPoint file\n",
    "pptx_path = \"/Users/vaishnavikamisetti/Desktop/Big_data.pptx\"\n",
    "\n",
    "# Extract text from PowerPoint slides\n",
    "slides_text = extract_text_from_pptx(pptx_path)\n",
    "\n",
    "# Calculate importance scores for each slide using LDA\n",
    "importance_scores_lda = calculate_importance_lda(slides_text)\n",
    "\n",
    "# Calculate importance scores for each slide based on structural elements\n",
    "importance_scores_structure = calculate_importance_structure(slides_text)\n",
    "\n",
    "# Calculate importance scores based on manual defined keywords\n",
    "importance_scores_keywords = []\n",
    "for slide in slides_text:\n",
    "    score = sum(1 for keyword in keywords if keyword.lower() in slide.lower())\n",
    "    importance_scores_keywords.append(score)\n",
    "\n",
    "# Combine importance scores from LDA, structural analysis, and keywords\n",
    "hybrid_importance_scores = np.add(np.add(importance_scores_lda, importance_scores_structure), importance_scores_keywords)\n",
    "\n",
    "# Print hybrid importance scores for each slide\n",
    "for i, importance in enumerate(hybrid_importance_scores, 1):\n",
    "    print(f\"Hybrid Importance of Slide {i}: {importance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b0e7899",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
