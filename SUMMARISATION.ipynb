{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f6f10ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b96d80",
   "metadata": {},
   "source": [
    "# SIMPLE SUMMARISER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "741056c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/apple/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Natural Language Processing\n",
      "CS 3216/UG, AI 5203/PG\n",
      "Week-5 Language models\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "15\n",
      "Types of Language Models\n",
      "Probabilistic language models (PLMs)\n",
      "\n",
      "Neural language models (NLMs)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pptx\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Load your PowerPoint data\n",
    "pptx_filepath = 'NLP.pptx'\n",
    "\n",
    "def extract_text_from_pptx(pptx_file):\n",
    "    text = []\n",
    "    prs = pptx.Presentation(pptx_file)\n",
    "    for slide in prs.slides:\n",
    "        slide_text = []\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\"):\n",
    "                slide_text.append(shape.text)\n",
    "        text.append(\"\\n\".join(slide_text))\n",
    "    return text\n",
    "\n",
    "slides_text = extract_text_from_pptx(pptx_filepath)\n",
    "\n",
    "# Preprocess your data\n",
    "corpus = []\n",
    "for text in slides_text:\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "    preprocessed_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokens = nltk.word_tokenize(sentence)\n",
    "        tokens = [word for word in tokens if word.lower() not in stop_words and len(word) > 1]\n",
    "        preprocessed_sentence = ' '.join(tokens)\n",
    "        preprocessed_sentences.append(preprocessed_sentence)\n",
    "    corpus.append(' '.join(preprocessed_sentences))\n",
    "\n",
    "# Convert the corpus to a matrix of token counts\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Calculate cosine similarity between each pair of sentences\n",
    "similarity_matrix = cosine_similarity(X, X)\n",
    "\n",
    "# Extract the most important sentences based on cosine similarity\n",
    "num_sentences = 2\n",
    "important_sentences_indices = similarity_matrix.argsort(axis=1)[:, ::-1][:, :num_sentences]\n",
    "important_sentences = [slides_text[index] for index in important_sentences_indices[0]]\n",
    "\n",
    "# Join the important sentences to form the summary\n",
    "summary = '\\n'.join(important_sentences)\n",
    "\n",
    "# Print the summary\n",
    "print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12d21f2",
   "metadata": {},
   "source": [
    "# SIMPLENN MODEL ON LESS TEXTUAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed39b306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6942\n",
      "Epoch [2/10], Loss: 0.6913\n",
      "Epoch [3/10], Loss: 0.6884\n",
      "Epoch [4/10], Loss: 0.6855\n",
      "Epoch [5/10], Loss: 0.6826\n",
      "Epoch [6/10], Loss: 0.6795\n",
      "Epoch [7/10], Loss: 0.6763\n",
      "Epoch [8/10], Loss: 0.6729\n",
      "Epoch [9/10], Loss: 0.6694\n",
      "Epoch [10/10], Loss: 0.6656\n",
      "Training finished!\n",
      "Combined Summary of All Slides:\n",
      "Natural Language Processing\n",
      "CS 3216/UG, AI 5203/PG\n",
      "Week-5 Language models\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2\n",
      "Recap\n",
      "NLP\n",
      "Applications\n",
      "Regular expressions\n",
      "Tokenization\n",
      "Stemming\n",
      "Porter Stemmer\n",
      "Lemmatization\n",
      "Normalization\n",
      "Stopwords\n",
      "Bag-of-Words\n",
      "TF-IDF\n",
      "NER\n",
      "POS tagging\n",
      "Semantics, Distributional semantics, Word2vec\n",
      "8\n",
      "Language Model\n",
      "Classic definition- Probability distribution\tover sequence of tokens\n",
      "Vocabulary V – a set of tokens!\n",
      "\n",
      "A language model p assigns each sequence of tokens (x1,….......xL) ∈\n",
      "V, a probability (a number between 0 and 1),\n",
      "P (x1, x2, x3, ............., xL)\n",
      "\n",
      "The probability intuitively tells us how \"good\" a sequence of tokens is.\n",
      "9\n",
      "Language Model\n",
      "Consider the probability of four strings in English.\n",
      "\n",
      "Here Vocabulary,\n",
      "\n",
      "V = {ate, ball, cheese, mouse, the}\n",
      "\n",
      "p(the, mouse, ate, the, cheese)=0.02\t------> P1\n",
      "p(the, cheese, ate, the, mouse)=0.01\t-------> P2\n",
      "p(mouse ,the, the, cheese, ate)=0.0001 -----> P3\n",
      "Clearly, P1 > P2 > P3\n",
      "Language Model\n",
      "\n",
      "LM assigned, \"mouse the the cheese ate\" a very low probability\n",
      "implicitly because it’s ungrammatical (syntactic knowledge).\n",
      "\n",
      "The LM should assign the mouse ate the cheese higher probability \tthan the cheese ate the mouse implicitly because of,\n",
      "\n",
      "world knowledge: both sentences are the same syntactically,\n",
      "10\n",
      "they differ in\n",
      "semantic plausibility\n",
      "Where do you see language models?\n",
      "Google Search system\n",
      "11\n",
      "Spell correction\n",
      "The office is about fifteen minuets from my house\n",
      "P(about fifteen minutes from) > P(about fifteen minuets from)\n",
      "12\n",
      "Machine Translation\n",
      "13\n",
      "P(high winds tonite) > P(large winds tonite)\n",
      "Speech recognition\n",
      "Here,\n",
      "14\n",
      "P(I saw a van) >> P(eyes awe of an)\n",
      "I saw a van\n",
      "15\n",
      "Types of Language Models\n",
      "Probabilistic language models (PLMs)\n",
      "\n",
      "Neural language models (NLMs)\n",
      "Credits: Slide adapted from [2]\n",
      "16\n",
      "Probabilistic Language Modeling\n",
      "Goal: compute the probability of a sentence or sequence of words:\n",
      "P(W) = P(w1,w2,w3,w4,w5,........wn )\n",
      "Related task: probability of an upcoming word:\n",
      "P(w5|w1,w2,w3,w4)\n",
      "A model that computes either of these:\n",
      "P(W) or P(wn|w1,w2….....wn-1)\n",
      "is called a language model.\n",
      "17\n",
      "Credits: Slide adapted from [2]\n",
      "How to compute P(W)\n",
      "How to compute this joint probability:\n",
      "\n",
      "P(its, water, is, so, transparent, that)\n",
      "\n",
      "Intuition: let’s rely on the Chain Rule of Probability\n",
      "Reminder: The Chain Rule\n",
      "\n",
      "Recall the definition of conditional probabilities\n",
      "\n",
      "P(B|A) = P(A,B)/P(A)\n",
      "\n",
      "Rewriting: P(A,B) = P(A)P(B|A)\n",
      "\n",
      "More variables:\n",
      "Credits: Slide adapted from [2]\n",
      "18\n",
      "P(A,B,C,D) = P(A)P(B|A)P(C|A,B)P(D|A,B,C)\n",
      "The Chain Rule in General\n",
      "\n",
      "P(x1,x2,x3,….........,xn) =  P(x1)P(x2|x1)P(x3|x1,x2)…P(xn|x1,…,xn-1)\n",
      "Chain Rule\n",
      "Credits: Slide adapted from [2]\n",
      "19\n",
      "To Compute joint probability of words in a sentence\n",
      "\"its water is so transparent\"\n",
      "How to estimate these probabilities\n",
      "Could we just count and divide?\n",
      "\n",
      "P(the |its water is so transparent that) =\n",
      "Count(its water is so transparent that the)\n",
      "\n",
      "Count(its water is so transparent that)\n",
      "Why Not?\n",
      "No!!!! Too many possible sentences!\n",
      "We will never see enough data to estimate these\n",
      "Credits: Slide adapted from [2]\n",
      "20\n",
      "Markov Assumption :\n",
      "Credits: Slide adapted from [2]\n",
      "21\n",
      "Markov Assumption\n",
      "Credits: Slide adapted from [2]\n",
      "22\n",
      "Simplest case: Unigram model\n",
      "Credits: Slide adapted from [2]\n",
      "23\n",
      "Bigram model\n",
      "Credits: Slide adapted from [2]\n",
      "24\n",
      "Estimating bigram probabilities\n",
      "Credits: Slide adapted from [2]\n",
      "25\n",
      "An example\n",
      "Credits: Slide adapted from [2]\n",
      "26\n",
      "Solution\n",
      "Credits: Slide adapted from [2]\n",
      "28\n",
      "Bigram estimates of sentence probabilities\n",
      "Credits: Slide adapted from [2]\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pptx\n",
    "\n",
    "# Load PowerPoint data\n",
    "def extract_text_from_pptx(pptx_file):\n",
    "    text = []\n",
    "    prs = pptx.Presentation(pptx_file)\n",
    "    for slide in prs.slides:\n",
    "        slide_text = []\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\"):\n",
    "                slide_text.append(shape.text)\n",
    "        text.append(\"\\n\".join(slide_text))\n",
    "    return text\n",
    "\n",
    "pptx_filepath = 'NLP.pptx'\n",
    "slides_text = extract_text_from_pptx(pptx_filepath)\n",
    "\n",
    "# Preprocess the slides\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = text.split()\n",
    "    # Remove stop words and short words\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    # Join tokens back into a string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "preprocessed_slides = [preprocess_text(text) for text in slides_text]\n",
    "\n",
    "# Combine preprocessed slides into a single document\n",
    "corpus = ' '.join(preprocessed_slides)\n",
    "\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(preprocessed_slides)\n",
    "\n",
    "# Convert TF-IDF matrix to PyTorch tensor\n",
    "tfidf_tensor = torch.tensor(tfidf_matrix.toarray(), dtype=torch.float32)\n",
    "\n",
    "# Define neural network model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Define training parameters\n",
    "input_size = tfidf_tensor.shape[1]  # Input size determined by TF-IDF matrix shape\n",
    "hidden_size = 128  # Hidden layer size\n",
    "output_size = 2  # Output size, e.g., binary classification (summary or not)\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Convert labels to tensor \n",
    "labels = torch.tensor([1] * (len(slides_text) // 2) + [0] * (len(slides_text) - len(slides_text) // 2), dtype=torch.long)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(tfidf_tensor)\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print('Training finished!')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'trained_model.pth')\n",
    "\n",
    "# Load the trained model\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "model.load_state_dict(torch.load('trained_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Generate summaries using the trained model\n",
    "summary_indices = torch.argmax(model(tfidf_tensor), dim=1).tolist()\n",
    "summaries = [slides_text[i] for i, label in enumerate(summary_indices) if label == 1]\n",
    "\n",
    "# Concatenate all summaries into a single string\n",
    "final_summary = '\\n'.join(summaries)\n",
    "\n",
    "# Print the final summary\n",
    "print(\"Combined Summary of All Slides:\")\n",
    "print(final_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a91453",
   "metadata": {},
   "source": [
    "# SIMPLENN MODEL ON MORE TEXTUAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf7bd9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.6921\n",
      "Epoch [2/10], Loss: 0.6883\n",
      "Epoch [3/10], Loss: 0.6846\n",
      "Epoch [4/10], Loss: 0.6810\n",
      "Epoch [5/10], Loss: 0.6773\n",
      "Epoch [6/10], Loss: 0.6735\n",
      "Epoch [7/10], Loss: 0.6697\n",
      "Epoch [8/10], Loss: 0.6657\n",
      "Epoch [9/10], Loss: 0.6615\n",
      "Epoch [10/10], Loss: 0.6572\n",
      "Training finished!\n",
      "Combined Summary of All Slides:\n",
      "Big Data Technologies\n",
      "Rahul Roy\n",
      "rahul.roy@mahindrauniversity.edu.in\n",
      "Functionality of Each Layer\n",
      "store masses of raw data from traditional sources like OLTP databases and newer, less structured sources like log files, sensors, web analytics, documents and media archives. Eg. Hadoop HDFS, Amazon S3, MongoDB\n",
      "\n",
      "This layer is responsible for collecting and storing data from various sources. The data ingestion process of extracting data from various sources and loading it into a data repository. Eg. Stitch, Apache Kafta, Blendo\n",
      "\n",
      "The data processing layer optimize the data to facilitate more efficient analysis, and provide a compute engine to run the queries. Ex. Spark, PostgreSQL, Amazon Redshift\n",
      "\n",
      "Using the technology in this layer, you can run queries to answer questions the business is asking, slice and dice the data, build dashboards and create beautiful visualizations, using one of many advanced BI tools. Ex. Tableau, ChartIO, Looker\n",
      "\n",
      "Big Data Architecture\n",
      "Case Study\n",
      "Reference: https://uber.com/en-IN/blog/uber-big-data-platform/\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pptx\n",
    "\n",
    "# Load PowerPoint data\n",
    "def extract_text_from_pptx(pptx_file):\n",
    "    text = []\n",
    "    prs = pptx.Presentation(pptx_file)\n",
    "    for slide in prs.slides:\n",
    "        slide_text = []\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\"):\n",
    "                slide_text.append(shape.text)\n",
    "        text.append(\"\\n\".join(slide_text))\n",
    "    return text\n",
    "\n",
    "pptx_filepath = 'Module2.pptx'\n",
    "slides_text = extract_text_from_pptx(pptx_filepath)\n",
    "\n",
    "# Preprocess the slides\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = text.split()\n",
    "    # Remove stop words and short words\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    # Join tokens back into a string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "preprocessed_slides = [preprocess_text(text) for text in slides_text]\n",
    "\n",
    "# Combine preprocessed slides into a single document\n",
    "corpus = ' '.join(preprocessed_slides)\n",
    "\n",
    "# TF-IDF vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(preprocessed_slides)\n",
    "\n",
    "# Convert TF-IDF matrix to PyTorch tensor\n",
    "tfidf_tensor = torch.tensor(tfidf_matrix.toarray(), dtype=torch.float32)\n",
    "\n",
    "# Define neural network model\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "# Define training parameters\n",
    "input_size = tfidf_tensor.shape[1]  # Input size determined by TF-IDF matrix shape\n",
    "hidden_size = 128  # Hidden layer size\n",
    "output_size = 2  # Output size, e.g., binary classification (summary or not)\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Initialize the model\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "labels = torch.tensor([1] * (len(slides_text) // 2) + [0] * (len(slides_text) - len(slides_text) // 2), dtype=torch.long)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(tfidf_tensor)\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print('Training finished!')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'trained_model.pth')\n",
    "\n",
    "# Load the trained model\n",
    "model = SimpleNN(input_size, hidden_size, output_size)\n",
    "model.load_state_dict(torch.load('trained_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Generate summaries using the trained model\n",
    "summary_indices = torch.argmax(model(tfidf_tensor), dim=1).tolist()\n",
    "summaries = [slides_text[i] for i, label in enumerate(summary_indices) if label == 1]\n",
    "\n",
    "# Concatenate all summaries into a single string\n",
    "final_summary = '\\n'.join(summaries)\n",
    "\n",
    "# Print the final summary\n",
    "print(\"Combined Summary of All Slides:\")\n",
    "print(final_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4520035",
   "metadata": {},
   "source": [
    "# DISTILBERT SUMMARISER WITH LESS TEXTUAL DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "051a346a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/Desktop/PYHTON/anaconda3/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.6954\n",
      "Epoch [2/5], Loss: 0.6845\n",
      "Epoch [3/5], Loss: 0.6776\n",
      "Epoch [4/5], Loss: 0.6695\n",
      "Epoch [5/5], Loss: 0.6607\n",
      "Training finished!\n",
      "Final Summary:\n",
      "Natural Language Processing\n",
      "CS 3216/UG, AI 5203/PG\n",
      "Week-5 Language models\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "2\n",
      "Recap\n",
      "NLP\n",
      "Applications\n",
      "Regular expressions\n",
      "Tokenization\n",
      "Stemming\n",
      "Porter Stemmer\n",
      "Lemmatization\n",
      "Normalization\n",
      "Stopwords\n",
      "Bag-of-Words\n",
      "TF-IDF\n",
      "NER\n",
      "POS tagging\n",
      "Semantics, Distributional semantics, Word2vec\n",
      "Where do you see language models?\n",
      "Google Search system\n",
      "11\n",
      "Spell correction\n",
      "The office is about fifteen minuets from my house\n",
      "P(about fifteen minutes from) > P(about fifteen minuets from)\n",
      "12\n",
      "Machine Translation\n",
      "13\n",
      "P(high winds tonite) > P(large winds tonite)\n",
      "Speech recognition\n",
      "Here,\n",
      "14\n",
      "P(I saw a van) >> P(eyes awe of an)\n",
      "I saw a van\n",
      "15\n",
      "Types of Language Models\n",
      "Probabilistic language models (PLMs)\n",
      "\n",
      "Neural language models (NLMs)\n",
      "Reminder: The Chain Rule\n",
      "\n",
      "Recall the definition of conditional probabilities\n",
      "\n",
      "P(B|A) = P(A,B)/P(A)\n",
      "\n",
      "Rewriting: P(A,B) = P(A)P(B|A)\n",
      "\n",
      "More variables:\n",
      "Credits: Slide adapted from [2]\n",
      "18\n",
      "P(A,B,C,D) = P(A)P(B|A)P(C|A,B)P(D|A,B,C)\n",
      "The Chain Rule in General\n",
      "\n",
      "P(x1,x2,x3,….........,xn) =  P(x1)P(x2|x1)P(x3|x1,x2)…P(xn|x1,…,xn-1)\n",
      "Chain Rule\n",
      "Credits: Slide adapted from [2]\n",
      "19\n",
      "To Compute joint probability of words in a sentence\n",
      "\"its water is so transparent\"\n",
      "Markov Assumption :\n",
      "Credits: Slide adapted from [2]\n",
      "21\n",
      "Markov Assumption\n",
      "Credits: Slide adapted from [2]\n",
      "22\n",
      "Simplest case: Unigram model\n",
      "Credits: Slide adapted from [2]\n",
      "23\n",
      "Bigram model\n",
      "Credits: Slide adapted from [2]\n",
      "24\n",
      "Estimating bigram probabilities\n",
      "Credits: Slide adapted from [2]\n",
      "25\n",
      "An example\n",
      "Credits: Slide adapted from [2]\n",
      "26\n",
      "27\n",
      "Solution?\n",
      "Solution\n",
      "Credits: Slide adapted from [2]\n",
      "28\n",
      "Raw bigram counts\n",
      "Total- 9333 sentences\n",
      "Credits: Slide adapted from [2]\n",
      "29\n",
      "Bigram estimates of sentence probabilities\n",
      "Credits: Slide adapted from [2]\n",
      "30\n",
      "What kinds of knowledge?\n",
      "31\n",
      "Practical Issues\n",
      "32\n",
      "\n",
      "\n",
      "Generating text with a n-gram Language Model\n",
      "You can also use a Language Model to generate text\n",
      "\n",
      "today the \t\n",
      "\n",
      "condition on this\n",
      "\n",
      "\n",
      "get probability distribution\n",
      "sample\n",
      "\n",
      "…\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pptx\n",
    "\n",
    "# Load PowerPoint data\n",
    "def extract_text_from_pptx(pptx_file):\n",
    "    text = []\n",
    "    prs = pptx.Presentation(pptx_file)\n",
    "    for slide in prs.slides:\n",
    "        slide_text = []\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\"):\n",
    "                slide_text.append(shape.text)\n",
    "        text.append(\"\\n\".join(slide_text))\n",
    "    return text\n",
    "\n",
    "pptx_filepath = 'NLP.pptx'\n",
    "slides_text = extract_text_from_pptx(pptx_filepath)\n",
    "\n",
    "# Preprocess the slides\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = text.split()\n",
    "    # Remove stop words and short words\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    # Join tokens back into a string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "preprocessed_slides = [preprocess_text(text) for text in slides_text]\n",
    "\n",
    "# Load DistilBERT model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "distilbert = DistilBertModel.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize input slides\n",
    "input_ids = tokenizer.batch_encode_plus(preprocessed_slides, padding=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# Define neural network model\n",
    "class DistilBertSummarizer(nn.Module):\n",
    "    def __init__(self, distilbert):\n",
    "        super(DistilBertSummarizer, self).__init__()\n",
    "        self.distilbert = distilbert\n",
    "        self.fc = nn.Linear(distilbert.config.hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.distilbert(input_ids)[0]  # DistilBERT output\n",
    "        cls_output = outputs[:, 0, :]  # Take only the first token's output (CLS token)\n",
    "        scores = self.fc(cls_output)  # Pass through linear layer\n",
    "        return self.sigmoid(scores)  # Apply sigmoid activation\n",
    "\n",
    "# Define training parameters\n",
    "num_epochs = 5\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# Initialize the model\n",
    "model = DistilBertSummarizer(distilbert)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "labels = torch.tensor([1] * (len(slides_text) // 2) + [0] * (len(slides_text) - len(slides_text) // 2), dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(input_ids)\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print('Training finished!')\n",
    "# Generate summaries using the trained model\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    predicted_labels = torch.round(outputs).squeeze().tolist()\n",
    "\n",
    "# Combine summaries of all slides\n",
    "final_summary = \"\"\n",
    "for slide_text, label in zip(slides_text, predicted_labels):\n",
    "    if label == 1:\n",
    "        final_summary += slide_text + \"\\n\"\n",
    "\n",
    "# Print the final summary\n",
    "print(\"Final Summary:\")\n",
    "print(final_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d603307f",
   "metadata": {},
   "source": [
    "# DISTILBERT SUMMARISER WITH MORE TEXTUAL SLIDES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd881d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/Desktop/PYHTON/anaconda3/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.7004\n",
      "Epoch [2/5], Loss: 0.6524\n",
      "Epoch [3/5], Loss: 0.6217\n",
      "Epoch [4/5], Loss: 0.5936\n",
      "Epoch [5/5], Loss: 0.5631\n",
      "Training finished!\n",
      "Final Summary:\n",
      "Big Data Technologies\n",
      "Rahul Roy\n",
      "rahul.roy@mahindrauniversity.edu.in\n",
      "Big Data Technologies\n",
      "Big Data Stack\n",
      "Big Data Architecture\n",
      "Case Study\n",
      "Reference: https://uber.com/en-IN/blog/uber-big-data-platform/\n",
      "7\n",
      "\n",
      "\n",
      "Original Google Stack\n",
      "8\n",
      "\n",
      "\n",
      "Facebook Version of the Stack\n",
      "9\n",
      "\n",
      "\n",
      "Yahoo Version of the Stack\n",
      "10\n",
      "\n",
      "\n",
      "LinkedIn’s Version of the Stack\n",
      "11\n",
      "\n",
      "\n",
      "Cloudera Version of the Stack\n",
      "Big Data Technologies\n",
      "Cloudera is a commercial Hadoop distribution that includes enterprise-grade features such as Cloudera Manager for cluster management, integrated security, and data governance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "import pptx\n",
    "\n",
    "# Load PowerPoint data\n",
    "def extract_text_from_pptx(pptx_file):\n",
    "    text = []\n",
    "    prs = pptx.Presentation(pptx_file)\n",
    "    for slide in prs.slides:\n",
    "        slide_text = []\n",
    "        for shape in slide.shapes:\n",
    "            if hasattr(shape, \"text\"):\n",
    "                slide_text.append(shape.text)\n",
    "        text.append(\"\\n\".join(slide_text))\n",
    "    return text\n",
    "\n",
    "pptx_filepath = 'Module2.pptx'\n",
    "slides_text = extract_text_from_pptx(pptx_filepath)\n",
    "\n",
    "# Preprocess the slides\n",
    "def preprocess_text(text):\n",
    "    # Tokenize the text\n",
    "    tokens = text.split()\n",
    "    # Remove stop words and short words\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    # Join tokens back into a string\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text\n",
    "\n",
    "preprocessed_slides = [preprocess_text(text) for text in slides_text]\n",
    "\n",
    "# Load DistilBERT model and tokenizer\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_name)\n",
    "distilbert = DistilBertModel.from_pretrained(model_name)\n",
    "\n",
    "# Tokenize input slides\n",
    "input_ids = tokenizer.batch_encode_plus(preprocessed_slides, padding=True, return_tensors=\"pt\")[\"input_ids\"]\n",
    "\n",
    "# Define neural network model\n",
    "class DistilBertSummarizer(nn.Module):\n",
    "    def __init__(self, distilbert):\n",
    "        super(DistilBertSummarizer, self).__init__()\n",
    "        self.distilbert = distilbert\n",
    "        self.fc = nn.Linear(distilbert.config.hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        outputs = self.distilbert(input_ids)[0]  # DistilBERT output\n",
    "        cls_output = outputs[:, 0, :]  # Take only the first token's output (CLS token)\n",
    "        scores = self.fc(cls_output)  # Pass through linear layer\n",
    "        return self.sigmoid(scores)  # Apply sigmoid activation\n",
    "\n",
    "# Define training parameters\n",
    "num_epochs = 5\n",
    "learning_rate = 1e-5\n",
    "\n",
    "# Initialize the model\n",
    "model = DistilBertSummarizer(distilbert)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Convert labels to tensor (example: assuming the first half of the slides are summaries, the second half are not)\n",
    "labels = torch.tensor([1] * (len(slides_text) // 2) + [0] * (len(slides_text) - len(slides_text) // 2), dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    outputs = model(input_ids)\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print('Training finished!')\n",
    "# Generate summaries using the trained model\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    predicted_labels = torch.round(outputs).squeeze().tolist()\n",
    "\n",
    "# Combine summaries of all slides\n",
    "final_summary = \"\"\n",
    "for slide_text, label in zip(slides_text, predicted_labels):\n",
    "    if label == 1:\n",
    "        final_summary += slide_text + \"\\n\"\n",
    "\n",
    "# Print the final summary\n",
    "print(\"Final Summary:\")\n",
    "print(final_summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
