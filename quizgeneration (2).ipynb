{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**DYNAMIC QUIZ GENERATION USING POS TAGGING INCLUDING ANSWERS:**"
      ],
      "metadata": {
        "id": "ztQMMmUaCUUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from pptx import Presentation\n",
        "\n",
        "class SubjectiveTest:\n",
        "    def __init__(self, data, noOfQues):\n",
        "        self.question_pattern = [\n",
        "            \"Explain in detail \",\n",
        "            \"Define \",\n",
        "            \"Write a short note on \",\n",
        "            \"What do you mean by \"\n",
        "        ]\n",
        "        self.grammar = r\"\"\"\n",
        "            CHUNK: {<NN>+<IN|DT>*<NN>+}\n",
        "            {<NN>+<IN|DT>*<NNP>+}\n",
        "            {<NNP>+<NNS>*}\n",
        "        \"\"\"\n",
        "        self.summary = data\n",
        "        self.noOfQues = noOfQues\n",
        "\n",
        "    @staticmethod\n",
        "    def word_tokenizer(sequence):\n",
        "        word_tokens = []\n",
        "        for sent in nltk.sent_tokenize(sequence):\n",
        "            for w in nltk.word_tokenize(sent):\n",
        "                word_tokens.append(w)\n",
        "        return word_tokens\n",
        "\n",
        "    def create_vector(answer_tokens, tokens):\n",
        "        return np.array([1 if tok in answer_tokens else 0 for tok in tokens])\n",
        "\n",
        "    def cosine_similarity_score(vector1, vector2):\n",
        "        def vector_value(vector):\n",
        "            return np.sqrt(np.sum(np.square(vector)))\n",
        "        v1 = vector_value(vector1)\n",
        "        v2 = vector_value(vector2)\n",
        "        v1_v2 = np.dot(vector1, vector2)\n",
        "        return (v1_v2 / (v1 * v2)) * 100\n",
        "\n",
        "    def generate_test(self):\n",
        "        sentences = nltk.sent_tokenize(self.summary)\n",
        "        cp = nltk.RegexpParser(self.grammar)\n",
        "        question_answer_dict = dict()\n",
        "        for sentence in sentences:\n",
        "            tagged_words = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
        "            tree = cp.parse(tagged_words)\n",
        "            for subtree in tree.subtrees():\n",
        "                if subtree.label() == \"CHUNK\":\n",
        "                    temp = \"\"\n",
        "                    for sub in subtree:\n",
        "                        temp += sub[0]\n",
        "                        temp += \" \"\n",
        "                    temp = temp.strip()\n",
        "                    temp = temp.upper()\n",
        "                    if temp not in question_answer_dict:\n",
        "                        if len(nltk.word_tokenize(sentence)) > 20:\n",
        "                            question_answer_dict[temp] = sentence\n",
        "                    else:\n",
        "                        question_answer_dict[temp] += sentence\n",
        "        keyword_list = list(question_answer_dict.keys())\n",
        "        question_answer = []\n",
        "        for _ in range(int(self.noOfQues)):\n",
        "            rand_num = np.random.randint(0, len(keyword_list))\n",
        "            selected_key = keyword_list[rand_num]\n",
        "            answer = question_answer_dict[selected_key]\n",
        "            rand_num %= 4\n",
        "            question = self.question_pattern[rand_num] + selected_key + \".\"\n",
        "            question_answer.append({\"Question\": question, \"Answer\": answer})\n",
        "        que = []\n",
        "        ans = []\n",
        "        while len(que) < int(self.noOfQues):\n",
        "            rand_num = np.random.randint(0, len(question_answer))\n",
        "            if question_answer[rand_num][\"Question\"] not in que:\n",
        "                que.append(question_answer[rand_num][\"Question\"])\n",
        "                ans.append(question_answer[rand_num][\"Answer\"])\n",
        "            else:\n",
        "                continue\n",
        "        return que, ans\n",
        "\n",
        "def extract_text_from_ppt(ppt_file):\n",
        "    prs = Presentation(ppt_file)\n",
        "    text = \"\"\n",
        "    for slide in prs.slides:\n",
        "        for shape in slide.shapes:\n",
        "            if hasattr(shape, \"text\"):\n",
        "                text += shape.text + \"\\n\"\n",
        "    return text.strip()\n",
        "\n",
        "# Extract text from the PowerPoint presentation\n",
        "ppt_file = \"/content/Module2.pptx\"\n",
        "lecture_text = extract_text_from_ppt(ppt_file)\n",
        "\n",
        "# Generate Subjective test\n",
        "noOfQues = 5  # Number of questions to generate\n",
        "subjective_generator = SubjectiveTest(lecture_text, noOfQues)\n",
        "\n",
        "subjective_questions, subjective_answers = subjective_generator.generate_test()\n",
        "\n",
        "# Printing generated questions and answers\n",
        "print(\"Subjective Questions:\")\n",
        "for i, question in enumerate(subjective_questions):\n",
        "    print(f\"Question {i+1}: {question}\")\n",
        "    print(f\"Answer {i+1}: {subjective_answers[i]}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2QVAEJQcOuS",
        "outputId": "176c0da2-e9c1-42eb-fe1b-1349a52f0880"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subjective Questions:\n",
            "Question 1: Define DATA PROCESSING LAYER.\n",
            "Answer 1: Stitch, Apache Kafta, Blendo\n",
            "\n",
            "The data processing layer optimize the data to facilitate more efficient analysis, and provide a compute engine to run the queries.\n",
            "\n",
            "Question 2: Define CLUSTER MANAGEMENT.\n",
            "Answer 2: Big Data Technologies\n",
            "Cloudera is a commercial Hadoop distribution that includes enterprise-grade features such as Cloudera Manager for cluster management, integrated security, and data governance.\n",
            "\n",
            "Question 3: Define HADOOP ECOSYSTEM.\n",
            "Answer 3: It includes:\n",
            "\t– MapReduce – offline computing engine\n",
            "\t– HDFS – Hadoop distributed file system\n",
            "\t– HBase– online data access\n",
            "\n",
            "Hadoop Ecosystem: Internal software architecture\n",
            "Hadoop Ecosystem Projects includes:\n",
            "Hadoop Common utilities\n",
            "Avro: A data serialization system with scripting languages.\n",
            "\n",
            "Question 4: Define VIDEOS.\n",
            "Answer 4: Flexibility: can deal with any kind of dataset like structured(MySql Data), Semi-Structured(XML, JSON), Un-structured (Images and Videos) very efficiently.\n",
            "\n",
            "Question 5: Define AMAZON REDSHIFT.\n",
            "Answer 5: Spark, PostgreSQL, Amazon Redshift\n",
            "\n",
            "Using the technology in this layer, you can run queries to answer questions the business is asking, slice and dice the data, build dashboards and create beautiful visualizations, using one of many advanced BI tools.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DYNAMIC QUIZ GENERATION USING POS TAGGING**"
      ],
      "metadata": {
        "id": "oXNPR685CrfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from pptx import Presentation\n",
        "\n",
        "class SubjectiveTest:\n",
        "    def __init__(self, data, noOfQues):\n",
        "        self.question_pattern = [\n",
        "            \"Explain in detail \",\n",
        "            \"Define \",\n",
        "            \"Write a short note on \",\n",
        "            \"What do you mean by \"\n",
        "        ]\n",
        "        self.grammar = r\"\"\"\n",
        "            CHUNK: {<NN>+<IN|DT>*<NN>+}\n",
        "            {<NN>+<IN|DT>*<NNP>+}\n",
        "            {<NNP>+<NNS>*}\n",
        "        \"\"\"\n",
        "        self.summary = data\n",
        "        self.noOfQues = noOfQues\n",
        "\n",
        "    @staticmethod\n",
        "    def word_tokenizer(sequence):\n",
        "        word_tokens = []\n",
        "        for sent in nltk.sent_tokenize(sequence):\n",
        "            for w in nltk.word_tokenize(sent):\n",
        "                word_tokens.append(w)\n",
        "        return word_tokens\n",
        "\n",
        "    def generate_test(self):\n",
        "        sentences = nltk.sent_tokenize(self.summary)\n",
        "        cp = nltk.RegexpParser(self.grammar)\n",
        "        question_answer_dict = dict()\n",
        "        for sentence in sentences:\n",
        "            tagged_words = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
        "            tree = cp.parse(tagged_words)\n",
        "            for subtree in tree.subtrees():\n",
        "                if subtree.label() == \"CHUNK\":\n",
        "                    temp = \"\"\n",
        "                    for sub in subtree:\n",
        "                        temp += sub[0]\n",
        "                        temp += \" \"\n",
        "                    temp = temp.strip()\n",
        "                    temp = temp.upper()\n",
        "                    if temp not in question_answer_dict:\n",
        "                        if len(nltk.word_tokenize(sentence)) > 20:\n",
        "                            question_answer_dict[temp] = sentence\n",
        "                    else:\n",
        "                        question_answer_dict[temp] += sentence\n",
        "        keyword_list = list(question_answer_dict.keys())\n",
        "        question_answer = []\n",
        "        for _ in range(int(self.noOfQues)):\n",
        "            rand_num = np.random.randint(0, len(keyword_list))\n",
        "            selected_key = keyword_list[rand_num]\n",
        "            rand_num %= 4\n",
        "            question = self.question_pattern[rand_num] + selected_key + \".\"\n",
        "            question_answer.append(question)\n",
        "        return question_answer\n",
        "\n",
        "def extract_text_from_ppt(ppt_file):\n",
        "    prs = Presentation(ppt_file)\n",
        "    text = \"\"\n",
        "    for slide in prs.slides:\n",
        "        for shape in slide.shapes:\n",
        "            if hasattr(shape, \"text\"):\n",
        "                text += shape.text + \"\\n\"\n",
        "    return text.strip()\n",
        "\n",
        "# Extract text from the PowerPoint presentation\n",
        "ppt_file = \"/content/Module2.pptx\"\n",
        "lecture_text = extract_text_from_ppt(ppt_file)\n",
        "\n",
        "# Generate Subjective test\n",
        "noOfQues = 5  # Number of questions to generate\n",
        "subjective_generator = SubjectiveTest(lecture_text, noOfQues)\n",
        "\n",
        "subjective_questions = subjective_generator.generate_test()\n",
        "\n",
        "# Printing generated questions\n",
        "print(\"Subjective Questions:\")\n",
        "for i, question in enumerate(subjective_questions):\n",
        "    print(f\"Question {i+1}: {question}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dP_wPj8NfQVR",
        "outputId": "67b1d663-26d2-4128-cb3a-e7b1246f5924"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subjective Questions:\n",
            "Question 1: What do you mean by APACHE HADOOP.\n",
            "\n",
            "Question 2: Define YARN.\n",
            "\n",
            "Question 3: Write a short note on DISTRIBUTION FOR HADOOP.\n",
            "\n",
            "Question 4: Explain in detail PRODUCTION USE.\n",
            "\n",
            "Question 5: Explain in detail BIG DATA.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from pptx import Presentation\n",
        "\n",
        "class SubjectiveTest:\n",
        "    def __init__(self, data, noOfQues):\n",
        "        self.question_pattern = [\n",
        "            \"Explain in detail \",\n",
        "            \"Define \",\n",
        "            \"Write a short note on \",\n",
        "            \"What do you mean by \"\n",
        "        ]\n",
        "        self.grammar = r\"\"\"\n",
        "            CHUNK: {<NN>+<IN|DT>*<NN>+}\n",
        "            {<NN>+<IN|DT>*<NNP>+}\n",
        "            {<NNP>+<NNS>*}\n",
        "        \"\"\"\n",
        "        self.summary = data\n",
        "        self.noOfQues = noOfQues\n",
        "\n",
        "    @staticmethod\n",
        "    def word_tokenizer(sequence):\n",
        "        word_tokens = []\n",
        "        for sent in nltk.sent_tokenize(sequence):\n",
        "            for w in nltk.word_tokenize(sent):\n",
        "                word_tokens.append(w)\n",
        "        return word_tokens\n",
        "\n",
        "    def generate_test(self):\n",
        "        sentences = nltk.sent_tokenize(self.summary)\n",
        "        cp = nltk.RegexpParser(self.grammar)\n",
        "        question_answer_dict = dict()\n",
        "        for sentence in sentences:\n",
        "            tagged_words = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
        "            tree = cp.parse(tagged_words)\n",
        "            for subtree in tree.subtrees():\n",
        "                if subtree.label() == \"CHUNK\":\n",
        "                    temp = \"\"\n",
        "                    for sub in subtree:\n",
        "                        temp += sub[0]\n",
        "                        temp += \" \"\n",
        "                    temp = temp.strip()\n",
        "                    temp = temp.upper()\n",
        "                    if temp not in question_answer_dict:\n",
        "                        if len(nltk.word_tokenize(sentence)) > 20:\n",
        "                            question_answer_dict[temp] = sentence\n",
        "                    else:\n",
        "                        question_answer_dict[temp] += sentence\n",
        "        keyword_list = list(question_answer_dict.keys())\n",
        "        question_answer = []\n",
        "        for _ in range(int(self.noOfQues)):\n",
        "            rand_num = np.random.randint(0, len(keyword_list))\n",
        "            selected_key = keyword_list[rand_num]\n",
        "            rand_num %= 4\n",
        "            question = self.question_pattern[rand_num] + selected_key + \".\"\n",
        "            question_answer.append(question)\n",
        "        return question_answer\n",
        "\n",
        "def extract_text_from_ppt(ppt_file):\n",
        "    prs = Presentation(ppt_file)\n",
        "    text = \"\"\n",
        "    for slide in prs.slides:\n",
        "        for shape in slide.shapes:\n",
        "            if hasattr(shape, \"text\"):\n",
        "                text += shape.text + \"\\n\"\n",
        "    return text.strip()\n",
        "\n",
        "# Extract text from the PowerPoint presentation\n",
        "ppt_file = \"/content/PPT 2=Lecture 2 Types and Evolution of Money.pptx\"\n",
        "lecture_text = extract_text_from_ppt(ppt_file)\n",
        "\n",
        "# Generate Subjective test\n",
        "noOfQues = 5  # Number of questions to generate\n",
        "subjective_generator = SubjectiveTest(lecture_text, noOfQues)\n",
        "\n",
        "subjective_questions = subjective_generator.generate_test()\n",
        "\n",
        "# Printing generated questions\n",
        "print(\"Subjective Questions:\")\n",
        "for i, question in enumerate(subjective_questions):\n",
        "    print(f\"Question {i+1}: {question}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "outputId": "7ee00ff5-d04d-4c74-8c63-2ad5bf1ab0df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8xNN6oe0lbFd"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subjective Questions:\n",
            "Question 1: Define LEGAL.\n",
            "\n",
            "Question 2: Explain in detail LEGAL TENDER MONEY.\n",
            "\n",
            "Question 3: Define GRESHAM.\n",
            "\n",
            "Question 4: What do you mean by PAYMENT MESSAGE.\n",
            "\n",
            "Question 5: Write a short note on MONEY E-MONEY.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "from pptx import Presentation\n",
        "\n",
        "class SubjectiveTest:\n",
        "    def __init__(self, data):\n",
        "        self.question_pattern = [\n",
        "            \"Explain in detail \",\n",
        "            \"Define \",\n",
        "            \"Write a short note on \",\n",
        "            \"What do you mean by \"\n",
        "        ]\n",
        "        self.grammar = r\"\"\"\n",
        "            CHUNK: {<NN>+<IN|DT>*<NN>+}\n",
        "            {<NN>+<IN|DT>*<NNP>+}\n",
        "            {<NNP>+<NNS>*}\n",
        "        \"\"\"\n",
        "        self.summary = data\n",
        "\n",
        "    @staticmethod\n",
        "    def word_tokenizer(sequence):\n",
        "        word_tokens = []\n",
        "        for sent in nltk.sent_tokenize(sequence):\n",
        "            for w in nltk.word_tokenize(sent):\n",
        "                word_tokens.append(w)\n",
        "        return word_tokens\n",
        "\n",
        "    def generate_test(self, noOfQues=5):\n",
        "        sentences = nltk.sent_tokenize(self.summary)\n",
        "        cp = nltk.RegexpParser(self.grammar)\n",
        "        question_answer_dict = dict()\n",
        "        for sentence in sentences:\n",
        "            tagged_words = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
        "            tree = cp.parse(tagged_words)\n",
        "            for subtree in tree.subtrees():\n",
        "                if subtree.label() == \"CHUNK\":\n",
        "                    temp = \"\"\n",
        "                    for sub in subtree:\n",
        "                        temp += sub[0]\n",
        "                        temp += \" \"\n",
        "                    temp = temp.strip()\n",
        "                    temp = temp.upper()\n",
        "                    if temp not in question_answer_dict:\n",
        "                        if len(nltk.word_tokenize(sentence)) > 20:\n",
        "                            question_answer_dict[temp] = sentence\n",
        "                    else:\n",
        "                        question_answer_dict[temp] += sentence\n",
        "        keyword_list = list(question_answer_dict.keys())\n",
        "        question_answer = []\n",
        "        for _ in range(int(noOfQues)):\n",
        "            rand_num = np.random.randint(0, len(keyword_list))\n",
        "            selected_key = keyword_list[rand_num]\n",
        "            rand_num %= 4\n",
        "            question = self.question_pattern[rand_num] + selected_key + \".\"\n",
        "            question_answer.append(question)\n",
        "        return question_answer\n",
        "\n",
        "def extract_text_from_ppt(ppt_file):\n",
        "    prs = Presentation(ppt_file)\n",
        "    text = \"\"\n",
        "    for slide in prs.slides:\n",
        "        for shape in slide.shapes:\n",
        "            if hasattr(shape, \"text\"):\n",
        "                text += shape.text + \"\\n\"\n",
        "    return text.strip()\n",
        "\n",
        "# Extract text from the PowerPoint presentation\n",
        "ppt_file = \"/content/Module2.pptx\"\n",
        "lecture_text = extract_text_from_ppt(ppt_file)\n",
        "\n",
        "# Generate Subjective test\n",
        "noOfQues = int(input(\"Enter the number of questions to generate: \"))  # Number of questions to generate\n",
        "subjective_generator = SubjectiveTest(lecture_text)\n",
        "\n",
        "subjective_questions = subjective_generator.generate_test(noOfQues)\n",
        "\n",
        "# Printing generated questions\n",
        "print(\"Subjective Questions:\")\n",
        "for i, question in enumerate(subjective_questions):\n",
        "    print(f\"Question {i+1}: {question}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTUezvA_xfS6",
        "outputId": "3713be72-097b-4deb-ca50-9504dfcfe204"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the number of questions to generate: 7\n",
            "Subjective Questions:\n",
            "Question 1: What do you mean by APACHE KAFTA.\n",
            "\n",
            "Question 2: What do you mean by BI TOOLS.\n",
            "\n",
            "Question 3: Explain in detail BIG DATA.\n",
            "\n",
            "Question 4: Explain in detail PRODUCTION USE.\n",
            "\n",
            "Question 5: Write a short note on – MAPREDUCE –.\n",
            "\n",
            "Question 6: Explain in detail OPEN SOURCE.\n",
            "\n",
            "Question 7: Explain in detail SEMI-STRUCTURED.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}